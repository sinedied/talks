<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>LangChain.js + Azure - A Generative AI App Journey</title>
  <link rel="stylesheet" href="style.css">
  <script src="remark.min.js"></script>
  <script src="rough-notation.js"></script>
  <script>
    function create() {
      var slideshow = remark.create({
        source: "title: LangChain.js + Azure - A Generative AI App Journey\nclass: animation-fade\nlayout: true\n\n<!-- .twitter-handle[\n  @sinedied\n] -->\n\n---\n\nclass: hide-handle, middle\nbackground-image: url(images/bg-main.jpg)\n\n# .text-tight.baseline.dark-text[LangChain.js + Azure .tiny.block[A Generative AI App Journey]]\n.dark-text[\nYohan Lasorsa\n]\n\n???\n- Hi everyone, I'm Yohan and I work as a senior cloud developer advocate in the JS team here at Microsoft\n\n- In this session, we'll explore how you can develop generative AI apps using LangChain.js and Azure\n\n---\n\n# Developing apps with Generative AI\n\n.center[\n<br><br><br>\n.w-80.responsive[![](images/cycle.png)]\n]\n\n???\n- We're still at the beginning of exploring what you can do with generative AI, and the field is moving fast, very fast!\n\n- When things move fast like this, you need to be able to experiment and iterate quickly. First to validate your ideas with a prototype, then to scale up to production if it works.\n\n---\n\nclass: light-gbg\n# Introducing LangChain.js ðŸ¦œðŸ”—\n\n.bit-larger.blist.baseline[\n- **Open-source JavaScript library for working with .blue-text[LLMs]**\n- **Provides high-level .blue-text[abstractions]:**\n  * models, vector databases, agents, and more\n- **Bridge between .blue-text[local and cloud-based] components**\n- **Allows complex .blue-text[compositions] of models and data**\n]\n\n???\n- So, let me introduce you to LangChain.js, a JavaScript library for working with large language models (LLMs). If the name sounds familiar, it's because it's the sister project of LangChain, which is made for Python, and a hugely popular open-source project in the AI community.\n\n- It's a great tool for building generative AI apps.\n\n---\n\n# Idea ðŸ’¡\n\n#### API for a Q&A system on YouTube video content\n\n.center[\n.w-80.responsive[![](images/qa-system.png)]\n]\n\n???\n- Let's start with a simple idea:\n- I'm from a generation who grew up reading books and articles, but nowadays most of the interesting content is in video format.\n\n- So, I thought it would be nice to have a system where you can ask questions about a YouTube video, and get answers quickly in a text format without having to watch the whole video.\n\n---\n\n# Retrieval-Augmented Generation (RAG)\n\n- **Combination of a .blue-text[retriever] and a .blue-text[generator]**\n- **Allows for more precise and relevant answers**\n\n.center[\n.w-70.responsive[![](images/rag.png)]\n]\n\n???\n- To implement this idea, we'll use this approach called Retrieval-Augmented Generation (RAG)\n\n- Why use RAG? Video content is often long and contains a lot of information. We want to be able to retrieve the most relevant information and then generate a precise answer to a question.\n\n- It grounds the answer in a set of documents, to avoid the generation of irrelevant or even made up information.\n\n---\n\n# Retrieval-Augmented Generation (RAG)\n.no-margin[\n#### Knowledge base builder\nCreate document embeddings from text data to be used in the retriever\n]\n.center[\n.w-70.responsive[![](images/embedding.png)]\n]\n???\n- Embedding: vector representation of a piece of data, that captures the underlying structure and relationships between pieces of data\n\n- Embeddings are used to build vector DBs\n\n- This is how we \"memorize\" things (ie your data) in LLMs\n\n- Example: law texts, medical texts, etc.\n\n---\n\n# Retrieval-Augmented Generation (RAG)\n.no-margin[\n#### Retrieval and context augmentation\nUse a vector DB to retrieve relevant information and add it to the context\n]\n.center[\n.w-75.responsive[![](images/augmentation.png)]\n]\n\n---\n\nclass: middle\nbackground-image: url(images/bg-demo.jpg)\n# Demo\n\n???\nNow, let's see how we can implement all of that using LangChain.js!\n\nFirst, we'll start with a local prototype, as it's easier and cheaper to experiment when working locally.\n\n#### Show Ollama\n\n- explain what ollama is\n  * run `ollama` command\n- `ollama list` => show installed models\n  * explain we'll use all-minilm:l6-v2 for embeddings and llama2 for the model\n- `ollama run llama2` => show the model\n  * \"How are you?\"\n  * It's a minimal version of ChatGPT running on your machine\n  * It also provide an API that you can use in your apps\n  * ctrl+d to exit\n\n#### Explain the prototype\n\n- open `prototype.js`\n  * explain the code step by step\n- `npm start` to run the demo\n\n#### Update the prototype to use Azure\n\n- replace model/db imports\n  * `imp` snippet\n- replace the model init sections\n  * use Copilot or `newai` snippet for the AI Search part\n  * explain that the models are defined in `.env` file (show the file)\n- replace the embeddings part\n  * `sea` snippet => first need to check if documents are already indexed\n  * `add` snippet (or use Copilot) to complete the embedding part\n- `npm start` to run the demo again\n\n#### Azure Functions\n\n- show `functions/lib/azure.ts`\n  * Explain changes to functions\n  * Async generator\n- show `functions/ask.ts`\n  * Talk about streaming support with v4 runtime\n  * Explain code\n  * Run locally and test with curl\n\n---\n\nclass: light-gbg\n# Wrap-up\n\n.bit-larger[\n**RAG implementation**<br>\nUsing LangChain.js abstractions and utilities\n]\n\n--\n.bit-larger[\n**Local prototype**<br>\nFAISS vector DB and Ollama + LLaMa2 7B / all-minilm-l6-v2 models\n]\n\n--\n.bit-larger[\n**Production on Azure**<br>\nAzure AI Search and GPT-4-turbo / text-embedding-3-large models\n]\n\n---\n\nclass: hidden-table, middle\nbackground-image: url(images/bg-main.jpg)\n# .large[Thank you!]\n\n- .w-35[Slides] [aka.ms/lcjs-azure](https://aka.ms/lcjs-azure)\n- .w-35[Demo source code] [aka.ms/lcjs-azure/demo](https://aka.ms/lcjs-azure/demo)\n- .w-35[Serverless ChatGPT sample] [aka.ms/lcjs/chat/sample](https://aka.ms/lcjs/chat/sample)\n- .w-35[Enterprise AI sample] [aka.ms/azai/js/code](https://aka.ms/azai/js/code)\n- .w-35[In-depth RAG workshop] [aka.ms/ws/openai-rag](https://aka.ms/ws/openai-rag)\n",
        ratio: '16:9',
        highlightLines: true,
        countIncrementalSlides: false,
        highlightStyle: 'github'
      });
      return slideshow;
    }
  </script>
</head>
<script src="code.js"></script>
<body onload="window.slideshow = create(); window.setup()">
</body>
</html>